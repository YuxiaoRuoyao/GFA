---
title: "Correcting for sample overlap"
author: "Jean Morrison"
date: "2020-02-24"
output: workflowr::wflow_html
editor_options:
  chunk_output_type: console
---

## Introduction
If many phenotypes are measured in the same sample or there is partial overlap between studies, there can be correlation in effect estimates that occurs as a result of sample phenotypic correlation rather than correlation in effect size. We would like to correct for this. This correlation occurs in all effect estimates, so it can be estimated using variants that are unlikely to affect any of the phenotypes. We can use this estimate to transform the matrix of effect estimates. Write the decomposition as

$$
\hat{B}_{p\times M} = L_{K \times p}^{T}F_{K\times M} + \Theta_{p\times M} + E_{p\times M}
$$


$p$ is the number of variants, $M$ the number of phenotypes, $K$ the number of factors. $\Theta$ represents effects of variants on traits not mediated by factors. We assume these are independent and from some unknown distribution. $E$ is the matrix that we are thinking about in this document. This is the estimation error. If each phenotype is measured in a different sample then the elements of $E$, $e_{i,j}$ are indepent and $e_{i,j} \sim N(0, s_{i,j}^2)$ where $s_{i,j}$ is the estimated variance of $\hat{B}_{i,j}$ available in the GWAS. However, if samples overlap, these are not independent and 
$$
Cov(E_{i, \cdot}) = \Sigma_{i (M\times M)} = S_{i}RS_{i}
$$
where $R$ is a correlation matrix that can be estimated as described above and $S_i$ is a diagonal matrix $M\times M$ matrix with elements $s_{i,j}$. We are assuming that elements in the same row of $E$ are correlated but that elements in different rows are indendent ($Cov(E_{i,j}, E_{i^\prime, j^\prime}) = 0$ if $i \neq i^\prime$).

It helps to write the relationship above for only one variant, $i$, here also transposing for convenience. 

$$
\hat{B}_{i (M\times 1)} = F_{(M\times K)}^{\top}L_{i (K\times 1)} + \Theta_{i} + E_i
$$

In this case, if $R$ is known then we can compute the eigen decomposition of $\Sigma_i = S_i R S_i = V_i D_i V_i^\top$ and find that $V_i^\top\hat{B}_i$ is a vector with independent elements. This is messy though and rquires us to do $p$ eigen decompositions. Further, replacing $\hat{B}$ with a matrix where rows are replaced by $V_i^\top \hat{B}_i$ won't necessarily give a matrix with the same low rank decomposition. If we assume that $s_{ij}$ can be decomposed as $s_i s_j$ then things are a bit easier. This is a reasonable approximation if variant allele frequencies are the same/similar across traits and the GWAS sample size for a single trait is the same/similar across variants because we can assume that $s_{ij} \propto \frac{1}{N_j f_i(1-f_i)}$. In this case $z$-scores have the same low-rank structure as the effect sizes (up to a scaling of the columns of $F$ and $L$) so we can write 

$$
\hat{Z}_{i (M\times 1)} = \tilde{F}_{(M\times K)}^{\top}\tilde{L}_{i (K\times 1)} + \tilde{\Theta}_{i} + \tilde{E}_i
$$
where $Cov(\tilde{E}_i) = R$. So now, with the eigen-decomposition of $R = UDU^\top$, we can replace $\hat{Z}$ with 

$$
\tilde{Z} = \hat{Z}U
$$
giving
$$
\tilde{Z} = L^T F U + \tilde{\Theta} U + \tilde{E}U
$$
where the elements of $\tilde{E}U$ are independent with variance $d_j$. We could also take $\tilde{Z} = \hat{Z}_{p\times M} D^{-1/2} U$ to have elements of $\tilde{E}D^{-1/2}U$ be iid stanard normal. One issue here is that we will end up estimating $FU$ rather than $F$ but it is possible that $FU$ is not sparse. We also now have $\tilde{\Theta}U$ which may have less structure thatn $\tilde{\Theta}$.

## Goals

My goals in this document are 

1. See how much correlation affects estimates of $F$
2. Verify that the strategy above works for recovering $F$ correctly using the oracle value of $R$.
3. See how well we do estimating $R$ from non-effect variants. 

## Simulating Data with Correlation Due to Sample Overlap

The first set of simulations is the same setup as used previously but now with $R$. We have 1000 variants, 10, traits and 3 hidden factors. $\Theta = 0_{M\times M}$ and $S = 1_{M\times M}$. 
```{r, message=FALSE, warning=FALSE}
library(flashier)
library(sumstatFactors)
library(tidyverse)
library(reshape2)
library(gridExtra)
```

```{r}
set.seed(1)
nvar <- 1000
ntrait <- 10
nfactor <- 3

S <- matrix(1, nrow=nvar, ncol=ntrait)

rloadings <- function(n){
  sigma_1 <- 1
  p <- 0.5
  load_dist <- ashr::normalmix(pi=c(1-p, p), mean=rep(0, 2), sd=c(0, sigma_1))
  return(causeSims::rnormalmix(n, load_dist))
}

rfactors <- function(n){
  sigma_1 <- 1
  p <- 0.5
  fact_dist <- ashr::normalmix(pi=c(1-p, p), mean=rep(0, 2), sd=c(0, sigma_1))
  return(causeSims::rnormalmix(n, fact_dist))
}
true_L <- replicate(n=nfactor, rloadings(nvar))
true_F <- replicate(n=nfactor, rfactors(ntrait))
true_Theta <- matrix(0, nrow=nvar, ncol=ntrait)

## Generate random correlation matrix
A <- matrix(rnorm(n=ntrait*ntrait), nrow=ntrait)
B <- A%*%t(A)
R <- cov2cor(B)

mats <- sim_bh2(true_L, true_F, true_Theta, S, R)
true_B = true_L%*%t(true_F)
### Plot the factors
ptrue <- plot_factors(true_F, 1:10)
ptrue
```


Try to recover factors without accounting for correlation

```{r}
#fit_naive <- run_flashier(mats, var_type="zero", init_type="soft_impute")
fit_naive <- run_flashier(mats, var_type="noisy_byrow", init_type="soft_impute")
p2 <- plot_factors(fit_naive$loadings.pm[[2]], 1:10)
grid.arrange(ptrue, p2, ncol=2)
gao_stability_sparse(mats$true_F, fit_naive$loadings.pm[[2]])
rrmse(Bhat = fitted(fit_naive), B = true_B)
```

This is actually not bad. Now with correction.

```{r}
R_eig <- eigen(R)
#U <- R_eig$vectors
U <- R_eig$vectors %*% diag(1/sqrt(R_eig$values))
mats$beta_hat <- mats$beta_hat %*% U
#mats$se_hat <- matrix(rep(sqrt(R_eig$values), nvar), nrow=nvar, byrow=T)
#fit_corrected <- run_flashier(mats, var_type="zero", init_type="soft_impute")
fit_corrected <- run_flashier(mats, var_type="noisy_byrow", init_type="soft_impute")
p3 <- plot_factors(U%*%fit_corrected$loadings.pm[[2]], 1:10)
grid.arrange(ptrue, p3, ncol=2)
gao_stability_sparse(mats$true_F, U %*% fit_corrected$loadings.pm[[2]])
rrmse(Bhat = fitted(fit_corrected)%*%solve(U), B = true_B)
```

```{r}
grid.arrange(ptrue, p2, p3)
```

We may get more distortion if $R$ has blocks of correlated traits rather than being random. Lets try with one group of three and one group of 4 highly correlated phenotypes

```{r}
R1 <- matrix(0.7, nrow=3, ncol=3)
diag(R1) <- 1
R2 <- matrix(0., nrow=4, ncol=4)
diag(R2) <- 1
R3 <- diag(rep(1, 3))
R <- Matrix::bdiag(R1, R2, R3) %>% as.matrix()

mats <- sim_bh2(true_L, true_F, true_Theta, S, R)

```

```{r}
#fit_naive <- run_flashier(mats, var_type="zero", init_type="soft_impute")
fit_naive <- run_flashier(mats, var_type="noisy_byrow", init_type="soft_impute")
p2 <- plot_factors(fit_naive$loadings.pm[[2]], 1:10)
grid.arrange(ptrue, p2, ncol=2)
gao_stability_sparse(mats$true_F, fit_naive$loadings.pm[[2]])
rrmse(Bhat = fitted(fit_naive), B = true_B)
```

```{r}
R_eig <- eigen(R)
#U <- R_eig$vectors
U <- R_eig$vectors %*% diag(1/sqrt(R_eig$values))
mats$beta_hat <- mats$beta_hat %*% U
#mats$se_hat <- matrix(rep(sqrt(R_eig$values), nvar), nrow=nvar, byrow=T)
#fit_corrected <- run_flashier(mats, var_type="zero", init_type="soft_impute")
fit_corrected <- run_flashier(mats, var_type="noisy_byrow", init_type="soft_impute")
p3 <- plot_factors(U%*%fit_corrected$loadings.pm[[2]], 1:10)
grid.arrange(ptrue, p3, ncol=2)
gao_stability_sparse(mats$true_F, U %*% fit_corrected$loadings.pm[[2]])
rrmse(Bhat = fitted(fit_corrected)%*%solve(U), B = true_B)
```


```{r}
corr_f <- cbind(rep(c(1, 0), c(3, 7)), rep(c(0, 1, 0), c(3, 4, 3)),
                rep(c(0, 1, 0), c(7, 1, 2)), rep(c(0, 1, 0), c(8, 1, 1)),
                rep(c(0, 1), c(9, 1)))
gao_stability_sparse(corr_f, fit_naive$loadings.pm[[2]])
gao_stability_sparse(corr_f, U %*% fit_corrected$loadings.pm[[2]])
```

Without the correction, one of the factors represents the structure of the correlation matrix. With the correction, this goes away. However, the correction probably matters more when more null variants are included.
This could be because nearly all of the variant included have some effect on at least one factor. Only 123 do not. We can see what is the effect of $R$ when there are more null variants and then see what is the effect when we select based on $p$-value. 


## More Null Variants
I wil regenerate the loadings matrix for more variants but many of them have no effects on any factor. We will keep using the same block diagonal $R$ matrix.

```{r, sim_select}
set.seed(2)
nvar <- 10000
rloadings <- function(n){
  sigma_1 <- 1
  p <- 0.1
  load_dist <- ashr::normalmix(pi=c(1-p, p), mean=rep(0, 2), sd=c(0, sigma_1))
  return(causeSims::rnormalmix(n, load_dist))
}
true_L <- replicate(n=nfactor, rloadings(nvar))
true_Theta <- matrix(0, nrow=nvar, ncol=ntrait)
S <- matrix(1, nrow=nvar, ncol=ntrait)

mats <- sim_bh2(true_L, true_F, true_Theta, S, R)
minp <- with(mats, apply(beta_hat/se_hat, 1,  function(z){
  p <- 2*pnorm(-abs(z))
  min(p)
}))
true_B = true_L%*%t(true_F)
```


```{r}
#fit_naive <- run_flashier(mats, var_type="zero", init_type="soft_impute")
fit_naive <- run_flashier(mats, var_type="noisy_byrow", init_type="soft_impute")
p2 <- plot_factors(fit_naive$loadings.pm[[2]], 1:10)
grid.arrange(ptrue, p2, ncol=2)
gao_stability_sparse(mats$true_F, fit_naive$loadings.pm[[2]])
rrmse(Bhat = fitted(fit_naive), B = true_B)
```
```

With many null SNPs we can see the effects of the sample correlation!

```{r}
R_eig <- eigen(R)
#U <- R_eig$vectors
U <- R_eig$vectors %*% diag(1/sqrt(R_eig$values))
mats$beta_hat <- mats$beta_hat %*% U
#mats$se_hat <- matrix(rep(sqrt(R_eig$values), nvar), nrow=nvar, byrow=T)
#fit_corrected <- run_flashier(mats, var_type="zero", init_type="soft_impute")
fit_corrected <- run_flashier(mats, var_type="noisy_byrow", init_type="soft_impute")
p3 <- plot_factors(U%*%fit_corrected$loadings.pm[[2]], 1:10)
grid.arrange(ptrue, p3, ncol=2)
gao_stability_sparse(mats$true_F, U %*% fit_corrected$loadings.pm[[2]])
rrmse(Bhat = fitted(fit_corrected)%*%solve(U), B = true_B)
```
```

The stability has gone down because we are missing a factor but importantly we no longer capture the factor that reflects the structure of the correlation matrix. 

```{r}
corr_f <- cbind(rep(c(1, 0), c(3, 7)), rep(c(0, 1, 0), c(3, 4, 3)),
                rep(c(0, 1, 0), c(7, 1, 2)), rep(c(0, 1, 0), c(8, 1, 1)),
                rep(c(0, 1), c(9, 1)))
gao_stability_sparse(corr_f, fit_naive$loadings.pm[[2]])
gao_stability_sparse(corr_f, U %*% fit_corrected$loadings.pm[[2]])
```
