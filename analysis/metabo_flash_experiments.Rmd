---
title: "Different ways to decompose summary statistics using EBMF"
author: "Jean Morrison"
date: "2019-08-26"
output: workflowr::wflow_html
editor_options:
  chunk_output_type: console
---

# Introduction

The goal of this document is to explore different ways to estimate a low rank decomposition of summary statistics. The example I will use is a set of 31 metabolic and growth related traits.


```{r, message=FALSE, warnings=FALSE, echo=FALSE, libraries}
library(flashr)
library(flashier)
library(sumstatFactors)
library(tidyverse)
library(reshape2)
knitr::opts_chunk$set(autodep = TRUE)
```


```{r, read_data}
mats <- readRDS("data/metabo3_gwas_mats.RDS")
mats$beta_hat[is.na(mats$se_hat)] <- NA
mats$se_hat[is.na(mats$beta_hat)] <- Inf
```

The data include `r nrow(mats$beta_hat)` SNPs that have a $p$-value less than $5 \cdot 10^{6}$ for at least one trait. Variants are pruned for LD at an $r^2$ threshold of 0.1, prefering variants with the lowest minimum $p$-value across all traits. The data have `r round(100*mean(is.na(mats$beta_hat)))`% missingness. We can compare different fits by 1) comparing the ELBO, 2) qualitatively comparing factors and 3) masking some of the non-missing data and estimating it. 

# `flashr` with `var_type="zero"`


Using FLASH with `var_type="zero"` fits the model
$$
Y = LF^T + E
$$
where $Y_{ij}$ is the effect estimate of variant $i$ on trait $j$ and $E_{ij} \sim N(0, s_{ij}^2)$ where $s_{ij}$ is the estimated standard error of $Y_{ij}$. This model assumes that $E[Y]$ is exactly low rank. 

We will start with the default point-normal prior for both factors and loadings. 

## SVD initialization

```{r, cache=TRUE, flash1}
kmax <- 100
data <- with(mats, flash_set_data(Y = beta_hat, S = se_hat))
f  <- flash_add_factors_from_data(data,K=kmax, var_type="zero") #, ebnm_fn = "ebnm_ash")
f <- flash_backfit(data,f, var_type="zero") 
```

## Plot

```{r, plot_flash1}
traits <- str_split(mats$traits, "/") %>% map(., 2) %>% 
          unlist(.) %>%
          str_replace(., ".top_summary_statistics.tsv.gz", "") %>%
          str_replace(., "metabo3_", "")
x <-f$ldf$f
meltx <- melt(x) %>%
         rename(Trait = Var1, Factor = Var2) %>%
         mutate( Trait = traits[Trait], 
                 Factor = as.factor(Factor))
ggplot(data = meltx, aes(x=Factor, y=Trait, fill=value)) + 
  geom_tile() + 
  scale_fill_gradient2() + 
  ggtitle('flashr with var_type="zero" and SVD factors')
```

I am particularly interested in birthweight and type 2 diabetes so I will comment on it here. There are two factors, 20 and 7 that have substantial weights on birth weight. Factor 7 appears to mostly affect "height related" traits including height, birth length, and head circumference but also has a small affect on insulin sensitivity. Factor 20 is also related to birth lenght and head circumference but is not associated with height and has opposite sign associations with type 2 diabetes risk and blood pressure. 

It is weakly associated with childhood bmi and childhood obesity. It is also worth noting that there are several factors which are nearly single trait factors. These include 
3 (coronary artery disease), 5 (type 2 diabetes), 11 (bone density), 16 (proinsulin), and 23 (waist to hip ratio)
 The objective for the fit is `r f$objective`. 

## Greedy initialization

On a suggestion from Jason I am trying a different initiailization of `flashr` that may be more similar to `flashier` below. Above we initialize factors from and SVD and then backfit. Instea, here we use the greedy algorithm and then backfit.

```{r, cache=TRUE, flash2}

f2  <- flash_add_greedy(data, var_type="zero", Kmax = kmax)
f2 <- flash_backfit(data,f2, var_type="zero")
```

As a check, this gives the same results as running `flashr::flash` with `backfit=TRUE`. 
```{r, eval=FALSE, flash2_copy}
f2_copy <- flash(data, var_type="zero", backfit=TRUE)
```

This strategy gives `r f2$nfactors` factors and a lower objective, `r f2$objective` than the SVD fit (objective: `r f$objective`). Here are the factors plotted

## Plot

```{r, plot_flash2}

x <-f2$ldf$f
meltx <- melt(x) %>%
         rename(Trait = Var1, Factor = Var2) %>%
         mutate( Trait = traits[Trait], 
                 Factor = as.factor(Factor))
ggplot(data = meltx, aes(x=Factor, y=Trait, fill=value)) + 
  geom_tile() + 
  scale_fill_gradient2()+ 
  ggtitle('flashr with var_type="zero" and greedy factors')
```

Similar patterns are seen with the two factors weighed on birth weight. There are fewer single trait factors and many factors are weighted on a large number of traits. 

# flashier

`flashier` is a faster implementation of FLASH that has some different variance type options. We will fit it with both `var.type=NULL` and `var.type=0`. `var.type=NULL` is equivalent to `var_type="zero"` in `flashr`. This wil serve as a comparision of the two packages. Using `var.type=0` fits the model 
$$
Y = LF^T + A + E
$$
where $Y$ and $E$ are as above and $A$ is a full rank matrix of additional errors with $A_{ij}$ iid $\sim N(0, \tau)$. This run uses the same default point normal prior as used above. 

```{r, cache=TRUE, flashier1}
fr1 <- with(mats, flashier(data=beta_hat, S = se_hat, var.type=NULL, greedy.Kmax = 100, init.tol=1e-4, fit = "full", verbose.lvl=3))
```

This results in `r fr1$n.factors` factors and an objective of `r fr1$elbo`. 


Using `var.type=0`
```{r, cache=TRUE, flashier2}
fr2 <- with(mats, flashier(data=beta_hat, S = se_hat, var.type=0, greedy.Kmax = 100, init.tol=1e-4, fit = "full", verbose.lvl=3))
```

# Summary

To summarize the number of factors and objective value from each method



Method                                  |  Number of Factors | Objective
----------------------------------------|--------------------|---------------
`flashr`,  SVD init, `var_type=0`       | `r f$nfactors`     | `r f$objective`
`flashr`, greedy init, `var_type=0`     | `r f2$nfactors`     | `r f2$objective`
`flashier`, greedy init, `var.type=NULL`| `r fr1$n.factors`  | `r fr1$elbo`
`flashier`, greedy init, `var.type=0`   | `r fr2$n.factors`  | `r fr2$elbo`


# Comparing methods by masking data

In addition to comparing the objectives, we can evaluate the fit by masking data and then computing the sum of squared differences between observed and imputed values. The code below does this 5 times for each method, masking a random set of 5\% of non-missing summary statistics each time.

```{r, message = FALSE, cache=TRUE, mask1}
mask_flash1 <- mask_and_fit(mats, fit_function = fit_flash_zero_svd, 
                            n_times=5, seed = 1)
```

```{r, hist1}
hist(mask_flash1, breaks=5)
cat("Sum of errors over 5 iterations: ", sum(mask_flash1), "\n")
```


```{r, message = FALSE, cache=TRUE, mask2}
mask_flash2 <- mask_and_fit(mats, fit_function = fit_flash_zero_greedy, 
                            n_times=5, seed = 1)
```

```{r, hist2}
hist(mask_flash2, breaks=5)
cat("Sum of errors over 5 iterations: ", sum(mask_flash2), "\n")
```
