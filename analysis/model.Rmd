---
title: "Model"
author: "Jean Morrison"
date: "2020-09-03"
output: workflowr::wflow_html
editor_options:
  chunk_output_type: console
---

## Data Generating Model

Here we show the details of our model for marginal GWAS summary statistics. 
Throughout $m$ indexes traits, $j$ indexes variants, $k$ indexes factors, and $i$ indexes individuals.
We are condiserding $M$ total traits and $J$ total variants. For each trait-variant pair, we observe the marginal association of variant $j$ with trait $m$ but we don't get to observe the individual level data. We denote the marginal effect estimate $\hat{\beta}_{m,j}$ and the corresponding standard error $\hat{s}_{m,j}$. 

We assume that these effect estimates originated from a GWAS, so within a trait effect estimates for all variants were computed using the same sample. Across traits, samples may overlap entirely, partially or not at all. 

We model $K$ ($K$ is unknown) unobserved underlying "factors". Each factor, $F_{k}$ has a linear effect  of $f_{m,k}$ on trait $m$. Each variant $G_j$ has a linear effect if $l_{j,k}$ on factor $k$. Vairant $j$ may have an additional linear effect on trait $m$, $\theta_{j,m}$ that is not mediated by factors. (See picture below)

Below is a list of assumptions we will use in this document. I have marked with a star assumptions that we can weaken or remove later. 

1. Every GWAS has appropriately corrected for population structure. Thuse every marginal effect measures either a causal effect of the variant on the trait or causal effects of nearby linked variants. *
2. All variants are independent (no LD). *
3. Effects of unobserved factors on traits are linear. 
4. Effects of variants on factors are linear.
5. Effects of factors on traits and variants on factors are sparse (many are equal to zero).
6. For a given $m$, $\theta_{j,m} \sim N(0, \sigma_{\theta, m}^2)$. $\theta_{j,m}$ and $\theta_{j^\prime, m^\prime}$ are independent if $j \neq j^\prime$ or $m \neq m^\prime$.
7. The elements of $L$ are IID with $Var(l_{j,k}) = \sigma^2_{l,k}$. 
8. The columns of $F$ have been scaled so that $\sum_{m=1}^M f_{m,k}^2 = 1$. Here we are thinking of $F$ as fixed and $L$ as random.

## GWAS with no overlapping samples

This is the simplest case. Using our previous assumptions, for a single individual $i$ in the study of trait $m$, we can write

$$
Y_{m,i} = \sum_{k=1}^{K}f_{mk}F_{k,i} + \sum_{j=1}^{J} \theta_{j,m}G^{(m)}_{j,i} + \epsilon_{m,i}
$$

where $\epsilon_{m,i}$ is any non-genetic component of $Y_{m,i}$ that is not mediated through the factors. The superscript $(m)$ on $G^{(m)}_{j,i}$ reminds us that these are the genotypes measured in study $m$. 

Each GWAS produces $\beta_{j, m}$, the association of variant $j$ with trait $m$. Using our assumption that all effect variants are independent, 

$$
\beta_{j,m} = \sum_{k=1}^{K}l_{jk}f_{mk} + \theta_{j,m}
$$
If each variant explains a small proportion of the overall trait variance, then the variance of $\hat{\beta}_{j,m}$, the OLS estimate of $\beta_{j,m}$ is approximately $\frac{\sigma^2_{Ym}}{N_m \sigma^2_{Gj}}$ where $\sigma^2_{Ym}$ is the variance of trait $m$ and $\sigma^2_{Gj}$ os the variance of variant $j$, or $2 f_j (1-f_j)$ where $f_j$ is the allele frequency of variant $j$. It will be convenient later to assume that both the traits and the variants have been standardized so that the variance of $\hat{\beta}_{j,m}$ is about $1/N_m$. We will refer to these as "standardized" effect estimates. 

Let $\hat{B}$ be the $J \times M$ matrix of marginal effect estimates and $B$ be the $J \times M$ matrix of true marginl associations. Then

$$
\hat{B} = B + E \\
 = L F^\top + \Theta + E
$$
where $L$ is the $J\times K$ matrix of SNP effects, $F$ is the $M \times K$ matrix of factor effects on traits and $\Theta$ is the $J\times M$ matrix of vairant effects not mediated by factors. 
Because all studies are independent and all variants are independent, the elements of $E$ are independent. If effects are standardized, we can assume that $E_{j,m} \sim N(0, \frac{1}{N_m})$, so each column of $E$ is iid. 

This is precisely the problem that FLASH is designed to solve, with additional assumptions that $L$ and $F$ are sparse and that $\Theta$ is dense. FLASH can estimate a different distribution for each collumn of $L$ and $F$ and each column of $\Theta$. 

## Fully overlapping samples

If samples fully overlap between the GWAS of all $M$ traits, the covariance of a row of $E$ will be $\frac{1}{N}\Sigma$ where $\Sigma$ is the $M \times M$ covariance of $Y_1, \dots, Y_m$. In practice, we will not know $\Sigma$ and will have to estimate it. However for the sake of generating realistic data and more fully characterizing the problem, it is worth considering the factors tha contribute to $\Sigma$. 

We can decompose $\Sigma$ into three components:

$$
\Sigma = \Sigma_G + \Sigma_{FE} + \Sigma_E
$$

$\Sigma_G$ is the genetic covariance while $\Sigma_{FE}$ and $\Sigma_E$ are non-genetic. We assume that shared factors may have both genetic and non-genetic effects and $\Sigma_{FE}$ represents the non-genetic (environmental) covariance due to shared factors. $\Sigma_E$ is environmental covariance not mediated by the heritble factors. We can write out expressions for each of these components:

$$
(\Sigma_G)_{m, m^\prime} = \sum_{j=1}^J E\left[\left(\sum_{k=1}^K l_{jk}f_{mk} + \theta_{jm}\right)\left(\sum_{k=1}^K l_{jk}f_{m^\prime k} + \theta_{jm^\prime}\right)\right]  \\
 = \sum_{j=1}^JE\left[\sum_{k =1}^{K} l_{j,k}^2 f_{m,k} f_{m^\prime, k}\right]  + E\left[\theta_{j,m}\theta_{j,m^\prime}\right]   \\
 = J \sum_{k=1}^K \sigma_{lk}^2  f_{m,k} f_{m^\prime, k} + JCov(\theta_m, \theta_{m^\prime})
 $$
Where $\sigma_{lk}^2 = Var(l_{j,k})$ and $Cov(\theta_m, \theta_{m^\prime}) = \sigma_{\theta,m}^2$ if $m = m^\prime$ and 0 otherwise. Here we are thinking of $F$ as fixed and $L$ as random. In matrix form

 $$
\Sigma_G = J (FV_LF^\top + V_\theta)
$$
where $V_\theta$ is a diagonal $M\times M$ matrix who's elements are equal to $\sigma^2_{\theta, m}$ and  $V_L$ is diagonal $K\times K$ with elements $\sigma^2_{l,k}$.

$\Sigma_{FE} = FV_{F}F^\top$ where $V_F$ is a $K \times K$ diagonal matrix who's elements are $\sigma^2_{FE,k}$. 

$\Sigma_E$ is the non-genetic, non-factor covariance and could be anything.

An important thing to note here is that $\Sigma_G$ and $\Sigma_{FE}$ are both have components that are $FDF^\top$ for some diagonal matrix D. 
This means we can re-write $E$ as 

$$
E = E_1 + E_2\\
= \tilde{L}F^\top + E_2
$$

where $\tilde{L}$ is a $J\times K$ matrix with independent elements $\tilde{L}_{j,k} \sim N(0, \omega_{k}^2)$ and the rows of $E_2$ have correlation due only to environmental covariance not mediated by factors. 

An important difference between $L$ and $\tilde{L}$ is that $L$ is sparse while $\tilde{L}$ is dense. However, if $L$ is not sparse enough it can be difficult to distinguish the two in estimation. 


## Partially overlapping samples

In practice we are likely to have varying degrees of sample overlap across different samples. This might be in blocks, for example a subset of traits are all from the same study (e.g. UK Biobank) while the remaining traits are independent. There might also be pairs of traits that have 
